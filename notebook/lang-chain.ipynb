{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ LangChain\n",
    "\n",
    "LLMのラッパー\n",
    "\n",
    "- [LangChain公式ドキュメント](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LangChain | GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "## # モジュール\n",
    "\n",
    "- [Modules | LangChain](https://python.langchain.com/docs/modules/)\n",
    "  - [Model I/O](https://python.langchain.com/docs/modules/model_io/)\n",
    "  - [Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "  - [Chains](https://python.langchain.com/docs/modules/chains)\n",
    "  - [Indexing](https://python.langchain.com/docs/modules/data_connection/indexing)\n",
    "  - [Memory](https://python.langchain.com/docs/modules/memory/)\n",
    "  - [Agents](https://python.langchain.com/docs/modules/agents/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Get Started\n",
    "\n",
    "- [Get started | LangChain](https://python.langchain.com/docs/expression_language/get_started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt, model, output parserを利用した基本的な例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'なぜ布団が10を数えるのが得意なのですか？\\n\\nだって、布団（ふとん）ですから！'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"user\", \"{topic} についてのジョークをおしえて\")\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ふとん\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Models モジュール\n",
    "\n",
    "LangChainで使用する機械学習のモデルのこと。以下の3種類がある。\n",
    "\n",
    "- LLMs  \n",
    "OpenAIのCompletions API(gpt-3.5-turbo-instructなど)の大規模言語モデル\n",
    "- Chat Models  \n",
    "OpenAIのChat API(gpt-4, gpt-3.5-turboなど)の大規模言語モデル\n",
    "- Text Embedding Models\n",
    "テキストをベクトル化するモデル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "- [OpenAI](https://python.langchain.com/docs/integrations/llms/openai/)\n",
    "\n",
    "内部的にはCompletionsAPIが利用される\n",
    "\n",
    "\n",
    "※ 現在はほぼほぼ使われない。ChatModelsの方を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "私は、山田太郎と申します。東京都出身で、現在は大学生として都内の大学に通っています。趣味はスポーツ観戦や音楽鑑賞で、特にサッカーやロックバンドが好きです。将来の夢は、国際的な企業で働くことで、留学経験も積んで自分を磨きたいと思っています。また、人とのコミュニケーションを大切にし、常に新しいことに挑戦することで成長していきたいと考えています。よろしくお願いします。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "result = llm.predict(\"自己紹介してください\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "- [ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai/)\n",
    "\n",
    "内部的には ChatAPIが利用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179390/1915069723.py:3: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm.predict(\"自己紹介してください\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！私はOpenAIが開発したAIアシスタントです。さまざまな質問に答えたり、情報を提供したり、アイデアを整理するお手伝いをしたりすることができます。趣味や興味、特定のトピックについてお話しすることもできますので、何か知りたいことや話したいことがあればお気軽にどうぞ！\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "result = llm.predict(\"自己紹介してください\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロンプトテンプレートとChainを利用する\n",
    "\n",
    "- [How to invoke runnables in parallel](https://python.langchain.com/docs/how_to/parallel/)\n",
    "\n",
    "## Prompt Templates\n",
    "プロンプトをテンプレート化することができる。  \n",
    "あくまで文字列を編集するだけでAPIをコールするわけではない。\n",
    "\n",
    "\n",
    "## Chain\n",
    "chainsは、モジュール(Models, Templates, Chainsなど)を連結する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ls` コマンドは、Unix系オペレーティングシステム（LinuxやmacOSなど）で使用されるコマンドで、指定したディレクトリ内のファイルやサブディレクトリの一覧を表示するために使われます。デフォルトでは、現在の作業ディレクトリの内容を表示します。\n",
      "\n",
      "主なオプションには以下のようなものがあります：\n",
      "\n",
      "- `-l`: 詳細情報を表示します（ファイルの権限、所有者、サイズ、最終更新日時など）。\n",
      "- `-a`: 隠しファイル（ドットで始まるファイル）も表示します。\n",
      "- `-h`: サイズを人間に読みやすい形式（KB、MBなど）で表示します（`-l`オプションと併用することが多い）。\n",
      "- `-R`: サブディレクトリも再帰的に表示します。\n",
      "\n",
      "このように、`ls` コマンドはファイルシステムの内容を把握するための基本的かつ重要なツールです。"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "次のコマンドの概要を説明してください。\n",
    "\n",
    "コマンド: {command}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "  HumanMessagePromptTemplate.from_template(template)\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# How to Streaming: https://python.langchain.com/docs/how_to/streaming/ \n",
    "stream = chain.stream({\"command\": \"ls\"})\n",
    "for message in stream:\n",
    "  print(message, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "llmの出力を次のllmの入力に渡したい場合はLangGraphを使うと便利\n",
    "\n",
    "- [LangGraph | 公式](https://langchain-ai.github.io/langgraph/)\n",
    "  - [Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n",
    "- [LangGraphの基本的な使い方 | Zenn](https://zenn.dev/pharmax/articles/8796b892eed183)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "#from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "cot_template = \"\"\"\n",
    "以下の質問に回答してください。\n",
    "\n",
    "### 質問 ###\n",
    "{question}\n",
    "### 質問終了 ###\n",
    "\n",
    "ステップバイステップで考えましょう。\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt = ChatPromptTemplate([\n",
    "  HumanMessagePromptTemplate.from_template(cot_template)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "summarize_template = \"\"\"\n",
    "入力を結論だけ一言に要約してください。\n",
    "\n",
    "### 入力 ###\n",
    "{answer}\n",
    "### 入力終了 ###\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate([\n",
    "  HumanMessagePromptTemplate.from_template(summarize_template)\n",
    "])\n",
    "\n",
    "\n",
    "### Graph ###\n",
    "\n",
    "class MyState(BaseModel):\n",
    "  question: str\n",
    "  answer: str = \"\"\n",
    "  summary: str = \"\"\n",
    "\n",
    "workflow = StateGraph(state_schema=MyState)\n",
    "\n",
    "def call_cot_chain(state: MyState):\n",
    "  chain = cot_prompt | model\n",
    "  response = chain.invoke({\"question\": state.question})\n",
    "  return {\"answer\": response.content}\n",
    "\n",
    "def call_summarize_chain(state: MyState):\n",
    "  chain = summarize_prompt | model\n",
    "  response = chain.invoke({\"answer\": state.answer})\n",
    "  return {\"summary\": response.content}\n",
    "\n",
    "\n",
    "workflow.add_node(\"cot\", call_cot_chain)\n",
    "workflow.add_node(\"summarize\", call_summarize_chain)\n",
    "\n",
    "workflow.add_edge(START, \"cot\")\n",
    "workflow.add_edge(\"cot\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終的に残っているリンゴの数は10個です。\n"
     ]
    }
   ],
   "source": [
    "output = app.invoke({\n",
    "    \"question\": \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\"\n",
    "})\n",
    "print(output[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Output Parsers\n",
    "\n",
    "- [Modules - Model I/O - Output Parsers | LangChain](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "LLMの応答を抽出して、Pythonオブジェクトにマッピングするといった機能を提供する。  \n",
    "他にもOutput Parserには、不正な形式でうまく解析できなかった場合に LLM に形式を整えてもらうような機能がある。  \n",
    "\n",
    "Promptsには他にも、Few-shotプロンプティングの例を埋め込むための「ExampleSelectors」という機能もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m料理のレシピを教えてください。\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"step\": {\"description\": \"steps to cook the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Step\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"step\"]}\n",
      "```\n",
      "\n",
      "料理名: カレー\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=== === === output === === ===\n",
      "```json\n",
      "{\n",
      "    \"ingredients\": [\n",
      "        \"玉ねぎ\",\n",
      "        \"にんじん\",\n",
      "        \"じゃがいも\",\n",
      "        \"牛肉\",\n",
      "        \"カレールー\",\n",
      "        \"水\"\n",
      "    ],\n",
      "    \"step\": [\n",
      "        \"1. 材料を準備する。\",\n",
      "        \"2. 玉ねぎ、にんじん、じゃがいもを切る。\",\n",
      "        \"3. 牛肉を炒める。\",\n",
      "        \"4. 野菜を加えて炒める。\",\n",
      "        \"5. 水を加えて煮込む。\",\n",
      "        \"6. カレールーを溶かして加える。\",\n",
      "        \"7. 煮込んで完成。\"\n",
      "    ]\n",
      "}\n",
      "```\n",
      "=== === === recipe === === ===\n",
      "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '牛肉', 'カレールー', '水'] step=['1. 材料を準備する。', '2. 玉ねぎ、にんじん、じゃがいもを切る。', '3. 牛肉を炒める。', '4. 野菜を加えて炒める。', '5. 水を加えて煮込む。', '6. カレールーを溶かして加える。', '7. 煮込んで完成。']\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "  ingredients: List[str] = Field(description=\"ingredients of the dish\")\n",
    "  step: List[str] = Field(description=\"steps to cook the dish\")\n",
    "\n",
    "template = \"\"\"料理のレシピを教えてください。\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "料理名: {dish}\n",
    "\"\"\"\n",
    "\n",
    "# 出力値のフォーマットを指示するためのプロンプトを生成する\n",
    "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"dish\"],\n",
    "  partial_variables={\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "  },\n",
    ")\n",
    "\n",
    "# Chain を実行\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "output = chain.run(dish=\"カレー\")\n",
    "print(\"=== === === output === === ===\")\n",
    "print(output)\n",
    "\n",
    "# 出力をPydanticオブジェクトにパース\n",
    "recipe = parser.parse(output)\n",
    "print(\"=== === === recipe === === ===\")\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes\n",
    "\n",
    "ChatGPTではコンテキストを指定して、その情報をベースに質問に回答させることができるが、文字数制限の関係ですべてのコンテキストを読み込ませることはできない。  \n",
    "この解決策として、文章をベクトル化して、Vector Storeに保存しておき、入力と近いベクトルの文章を検索して、context煮含めるといった手法用いる。  \n",
    "このような機能をカンタンに実装できるのがLangChainのInexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 130422, done.\u001b[K\n",
      "remote: Counting objects: 100% (14599/14599), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1214/1214), done.\u001b[K\n",
      "remote: Total 130422 (delta 13811), reused 13771 (delta 13384), pack-reused 115823\u001b[K\n",
      "Receiving objects: 100% (130422/130422), 173.75 MiB | 24.70 MiB/s, done.\n",
      "Resolving deltas: 100% (95827/95827), done.\n",
      "Updating files: 100% (6393/6393), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating files: 100% (8015/8015), done.\n",
      "Note: switching to 'v0.0.172'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at a63ab7ded bump 172 (#4864)\n"
     ]
    }
   ],
   "source": [
    "!cd langchain && git checkout v0.0.172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorstoreIndexCreator\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./langchain/docs\u001b[39m\u001b[38;5;124m\"\u001b[39m, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorstoreIndexCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:83\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_loaders\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loader \u001b[38;5;129;01min\u001b[39;00m loaders:\n\u001b[1;32m     82\u001b[0m     docs\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:88\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a vectorstore index from documents.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m sub_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m---> 88\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_kwargs\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m VectorStoreIndexWrapper(vectorstore\u001b[38;5;241m=\u001b[39mvectorstore)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:714\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    694\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid1()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:81\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a Chroma client.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/chromadb/__init__.py:79\u001b[0m\n\u001b[1;32m     77\u001b[0m             sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpysqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[91mYour system has an unsupported version of sqlite3. Chroma \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m                    requires sqlite3 >= 3.35.0.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[94mPlease visit \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m                    https://docs.trychroma.com/troubleshooting#sqlite to learn how \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m                    to upgrade.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override Chroma's default settings, environment variables or .env files\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(\"./langchain/docs\", glob=\"**/*.md\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
