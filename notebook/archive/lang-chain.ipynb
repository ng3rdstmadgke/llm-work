{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ LangChain\n",
    "\n",
    "LLMのラッパー\n",
    "\n",
    "- [LangChain公式ドキュメント](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LangChain | GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "## # モジュール\n",
    "\n",
    "- [Modules | LangChain](https://python.langchain.com/docs/modules/)\n",
    "  - [Model I/O](https://python.langchain.com/docs/modules/model_io/)\n",
    "  - [Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "  - [Chains](https://python.langchain.com/docs/modules/chains)\n",
    "  - [Indexing](https://python.langchain.com/docs/modules/data_connection/indexing)\n",
    "  - [Memory](https://python.langchain.com/docs/modules/memory/)\n",
    "  - [Agents](https://python.langchain.com/docs/modules/agents/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Get Started\n",
    "\n",
    "- [Get started | LangChain](https://python.langchain.com/docs/expression_language/get_started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt, model, output parserを利用した基本的な例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'なぜ布団が仕事を辞めたのですか？\\n\\n答え：だって、毎日押しつぶされていたからです。'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} についてのジョークをおしえて\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ふとん\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Models モジュール\n",
    "\n",
    "LangChainで使用する機械学習のモデルのこと。以下の3種類がある。\n",
    "\n",
    "- LLMs  \n",
    "OpenAIのCompletions API(gpt-3.5-turbo-instructなど)の大規模言語モデル\n",
    "- Chat Models  \n",
    "OpenAIのChat API(gpt-4, gpt-3.5-turboなど)の大規模言語モデル\n",
    "- Text Embedding Models\n",
    "テキストをベクトル化するモデル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "- [Modules - Model I/O - LLMS | LangChain公式](https://python.langchain.com/docs/modules/model_io/llms/)\n",
    "\n",
    "内部的にはCompletionsAPIが利用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "私は、山田太郎と申します。東京都出身で、現在は大学生として都内の大学に通っています。趣味はスポーツ観戦や音楽鑑賞で、特にサッカーやロックバンドが好きです。将来の夢は、国際的な企業で働くことで、留学経験も積んで自分を磨きたいと思っています。また、人とのコミュニケーションを大切にし、常に新しいことに挑戦することで成長していきたいと考えています。よろしくお願いします。\n"
     ]
    }
   ],
   "source": [
    "# LLMs\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "result = llm.predict(\"自己紹介してください\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "- [Modules - Model I/O - Chat Models | LangChain公式](https://python.langchain.com/docs/modules/model_io/chat/)\n",
    "\n",
    "内部的には ChatAPIが利用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はじめまして、私はAIアシスタントです。自然言語処理技術を用いて、様々な質問に回答したり、会話をすることができます。お手伝いが必要なことがあれば、遠慮なくお知らせください。どうぞよろしくお願いいたします。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "result = llm.predict(\"自己紹介してください\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Prompts モジュール\n",
    "\n",
    "- [Modules - Model I/O - Prompts | LangChain公式](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "\n",
    "モデルへの入力を組み立てるモジュール。以下の4つの要素がある。\n",
    "\n",
    "- Prompt Templetes  \n",
    "プロンプトをテンプレート化できる\n",
    "- Chat Prompt Templates\n",
    "- Example Selectors\n",
    "- Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "プロンプトをテンプレート化することができる。  \n",
    "あくまで文字列を編集するだけでAPIをコールするわけではない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "次のコマンドの概要を説明してください。\n",
      "\n",
      "コマンド: ls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "次のコマンドの概要を説明してください。\n",
    "\n",
    "コマンド: {command}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"command\"],\n",
    ")\n",
    "result = prompt.format(command=\"ls\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Chains モジュール\n",
    "\n",
    "- [Modules - Chains | LangChain公式](https://python.langchain.com/docs/modules/chains)\n",
    "\n",
    "chainsは、モジュール(Models, Templates, Chainsなど)を連結する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "次のコマンドの概要を説明してください。\n",
      "\n",
      "コマンド: ls\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "lsコマンドは、リスト（list）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。lsコマンドを実行することで、ファイルやディレクトリの名前や属性、更新日時などの情報を確認することができます。\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "langchain.verbose = True\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "次のコマンドの概要を説明してください。\n",
    "\n",
    "コマンド: {command}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"command\"],\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "result = chain.run(\"ls\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain\n",
    "\n",
    "ChainとChainを直列に連結できる。\n",
    "\n",
    "<img src=\"./img/simple_sequential_chain.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "以下の質問に回答してください。\n",
      "\n",
      "### 質問 ###\n",
      "私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\n",
      "### 質問終了 ###\n",
      "\n",
      "ステップバイステップで考えましょう。\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m1. 最初に市場で10個のリンゴを買いました。\n",
      "2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n",
      "3. その後、5つのリンゴを追加で購入しました。合計で6 + 5 = 11個のリンゴがあります。\n",
      "4. 1つ食べたので、残りは11 - 1 = 10個です。\n",
      "\n",
      "したがって、残りのリンゴは10個です。\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "入力を結論だけ一言に要約してください。\n",
      "\n",
      "### 入力 ###\n",
      "1. 最初に市場で10個のリンゴを買いました。\n",
      "2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n",
      "3. その後、5つのリンゴを追加で購入しました。合計で6 + 5 = 11個のリンゴがあります。\n",
      "4. 1つ食べたので、残りは11 - 1 = 10個です。\n",
      "\n",
      "したがって、残りのリンゴは10個です。\n",
      "### 入力終了 ###\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m残りのリンゴは10個。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "残りのリンゴは10個。\n"
     ]
    }
   ],
   "source": [
    "# 「ステップバイステップで考える(COT)」 と 「要約」 の組み合わせ\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "# 「ステップバイステップで考える(COT)」 Prompt と Chain を用意\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "cot_template = \"\"\"\n",
    "以下の質問に回答してください。\n",
    "\n",
    "### 質問 ###\n",
    "{question}\n",
    "### 質問終了 ###\n",
    "\n",
    "ステップバイステップで考えましょう。\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt = PromptTemplate(\n",
    "  template=cot_template,\n",
    "  input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n",
    "\n",
    "# 「要約」 Prompt と Chain を用意\n",
    "summarize_template = \"\"\"\n",
    "入力を結論だけ一言に要約してください。\n",
    "\n",
    "### 入力 ###\n",
    "{input}\n",
    "### 入力終了 ###\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "  input_variables=[\"input\"],\n",
    "  template=summarize_template,\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n",
    "\n",
    "\n",
    "# 2 つの Chain を直列に繋ぐ\n",
    "cot_summarize_chain = SimpleSequentialChain(\n",
    "  chains=[cot_chain, summarize_chain]\n",
    ")\n",
    "\n",
    "# 実行\n",
    "result = cot_summarize_chain(\n",
    "  \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\"\n",
    ")\n",
    "\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ■ Output Parsers\n",
    "\n",
    "- [Modules - Model I/O - Output Parsers | LangChain](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "LLMの応答を抽出して、Pythonオブジェクトにマッピングするといった機能を提供する。  \n",
    "他にもOutput Parserには、不正な形式でうまく解析できなかった場合に LLM に形式を整えてもらうような機能がある。  \n",
    "\n",
    "Promptsには他にも、Few-shotプロンプティングの例を埋め込むための「ExampleSelectors」という機能もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m料理のレシピを教えてください。\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"step\": {\"description\": \"steps to cook the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Step\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"step\"]}\n",
      "```\n",
      "\n",
      "料理名: カレー\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=== === === output === === ===\n",
      "```json\n",
      "{\n",
      "    \"ingredients\": [\n",
      "        \"玉ねぎ\",\n",
      "        \"にんじん\",\n",
      "        \"じゃがいも\",\n",
      "        \"牛肉\",\n",
      "        \"カレールー\",\n",
      "        \"水\"\n",
      "    ],\n",
      "    \"step\": [\n",
      "        \"1. 材料を準備する。\",\n",
      "        \"2. 玉ねぎ、にんじん、じゃがいもを切る。\",\n",
      "        \"3. 牛肉を炒める。\",\n",
      "        \"4. 野菜を加えて炒める。\",\n",
      "        \"5. 水を加えて煮込む。\",\n",
      "        \"6. カレールーを溶かして加える。\",\n",
      "        \"7. 煮込んで完成。\"\n",
      "    ]\n",
      "}\n",
      "```\n",
      "=== === === recipe === === ===\n",
      "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '牛肉', 'カレールー', '水'] step=['1. 材料を準備する。', '2. 玉ねぎ、にんじん、じゃがいもを切る。', '3. 牛肉を炒める。', '4. 野菜を加えて炒める。', '5. 水を加えて煮込む。', '6. カレールーを溶かして加える。', '7. 煮込んで完成。']\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "  ingredients: List[str] = Field(description=\"ingredients of the dish\")\n",
    "  step: List[str] = Field(description=\"steps to cook the dish\")\n",
    "\n",
    "template = \"\"\"料理のレシピを教えてください。\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "料理名: {dish}\n",
    "\"\"\"\n",
    "\n",
    "# 出力値のフォーマットを指示するためのプロンプトを生成する\n",
    "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"dish\"],\n",
    "  partial_variables={\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "  },\n",
    ")\n",
    "\n",
    "# Chain を実行\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "output = chain.run(dish=\"カレー\")\n",
    "print(\"=== === === output === === ===\")\n",
    "print(output)\n",
    "\n",
    "# 出力をPydanticオブジェクトにパース\n",
    "recipe = parser.parse(output)\n",
    "print(\"=== === === recipe === === ===\")\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes\n",
    "\n",
    "ChatGPTではコンテキストを指定して、その情報をベースに質問に回答させることができるが、文字数制限の関係ですべてのコンテキストを読み込ませることはできない。  \n",
    "この解決策として、文章をベクトル化して、Vector Storeに保存しておき、入力と近いベクトルの文章を検索して、context煮含めるといった手法用いる。  \n",
    "このような機能をカンタンに実装できるのがLangChainのInexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 130422, done.\u001b[K\n",
      "remote: Counting objects: 100% (14599/14599), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1214/1214), done.\u001b[K\n",
      "remote: Total 130422 (delta 13811), reused 13771 (delta 13384), pack-reused 115823\u001b[K\n",
      "Receiving objects: 100% (130422/130422), 173.75 MiB | 24.70 MiB/s, done.\n",
      "Resolving deltas: 100% (95827/95827), done.\n",
      "Updating files: 100% (6393/6393), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating files: 100% (8015/8015), done.\n",
      "Note: switching to 'v0.0.172'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at a63ab7ded bump 172 (#4864)\n"
     ]
    }
   ],
   "source": [
    "!cd langchain && git checkout v0.0.172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorstoreIndexCreator\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./langchain/docs\u001b[39m\u001b[38;5;124m\"\u001b[39m, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorstoreIndexCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:83\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_loaders\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loader \u001b[38;5;129;01min\u001b[39;00m loaders:\n\u001b[1;32m     82\u001b[0m     docs\u001b[38;5;241m.\u001b[39mextend(loader\u001b[38;5;241m.\u001b[39mload())\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:88\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a vectorstore index from documents.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m sub_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m---> 88\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore_kwargs\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m VectorStoreIndexWrapper(vectorstore\u001b[38;5;241m=\u001b[39mvectorstore)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:714\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    694\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid1()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:81\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a Chroma client.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/chromadb/__init__.py:79\u001b[0m\n\u001b[1;32m     77\u001b[0m             sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpysqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[91mYour system has an unsupported version of sqlite3. Chroma \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m                    requires sqlite3 >= 3.35.0.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[94mPlease visit \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m                    https://docs.trychroma.com/troubleshooting#sqlite to learn how \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m                    to upgrade.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override Chroma's default settings, environment variables or .env files\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \u001b[91mYour system has an unsupported version of sqlite3. Chroma                     requires sqlite3 >= 3.35.0.\u001b[0m\n\u001b[94mPlease visit                     https://docs.trychroma.com/troubleshooting#sqlite to learn how                     to upgrade.\u001b[0m"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(\"./langchain/docs\", glob=\"**/*.md\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
