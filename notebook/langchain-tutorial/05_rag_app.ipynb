{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Build a Retrieval Augmented Generation (RAG) App](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "テキスト データ ソースを使用して簡単な Q&A アプリケーションを構築する\n",
    "\n",
    "## ■ RAGのコンポーネント\n",
    "\n",
    "一般的な RAG アプリケーションには、次の 2 つの主要コンポーネントがあります。\n",
    "\n",
    "- **インデックス作成**  \n",
    "ソースからデータを取り込んでインデックスを作成するパイプライン。これは通常、オフラインで行われます。\n",
    "- **検索と生成**  \n",
    "実際の RAG チェーンは、実行時にユーザークエリを受け取り、インデックスから関連データを取得して、それをモデルに渡します。\n",
    "\n",
    "### # インデックス生成の流れ\n",
    "\n",
    "\n",
    "1. **ロード**  \n",
    "まずデータをロードする必要があります。これは[Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/)を使用して行われます。\n",
    "1. **分割**  \n",
    "[Text splitters](https://python.langchain.com/docs/concepts/text_splitters/)は、大きな `Documents` を小さなチャンクに分割します。  \n",
    "これは、データのインデックス作成とモデルへの受け渡しの両方に役立ちます。  \n",
    "大きなチャンクは検索が難しく、モデルの有限のコンテキストウィンドウに収まらないためです。  \n",
    "1. **保存**  \n",
    "分割したものを保存してインデックス化し、後で検索できるようにする場所が必要です。  \n",
    "これは通常、[VectorStore](https://python.langchain.com/docs/concepts/vectorstores/)と[Embeddings](https://python.langchain.com/docs/concepts/embedding_models/)モデルを使用して行われます。  \n",
    "\n",
    "<img src=\"../../docs/img/05_rag_app/01_indexing.png\" width=\"700px\">\n",
    "\n",
    "### # 検索と生成の流れ\n",
    "\n",
    "\n",
    "1. **検索**  \n",
    "ユーザ入力が与えられると、[Retriever](https://python.langchain.com/docs/concepts/retrievers/)が関連するスプリットを取得します。\n",
    "1. **生成**  \n",
    "[ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/) は質問と検索されたデータの両方を含むプロンプトを使って回答を生成します。\n",
    "\n",
    "<img src=\"../../docs/img/05_rag_app/02_retrieval.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プレビュー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'リポジトリは、ユーザー情報の保存、取得、更新、削除を担当する役割を持っています。具体的には、データを永続化するための手段として使用されます。たとえば、`UserRepository`はユーザー情報をJSONファイルに保存し、必要に応じてデータを取得、更新、削除します。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "\n",
    "from typing import List\n",
    "import bs4\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "#\n",
    "# 1. ロード\n",
    "#\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"it-MdContent\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://qiita.com/ktamido/items/b4ea0c40c5e1957904f6\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4_strainer\n",
    "    }\n",
    ")\n",
    "docs: List[Document] = loader.load()\n",
    "\n",
    "\n",
    "#\n",
    "# 2. 分割\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits: List[Document] = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "#\n",
    "# 3. 保存\n",
    "#\n",
    "vectorstore: Chroma = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "#\n",
    "# 4. 検索と生成\n",
    "#\n",
    "\n",
    "# モデル\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# プロンプト\n",
    "template = \"\"\"\n",
    "あなたは質問応答のアシスタントです。質問に答えるために、検索された文脈の以下の部分を使用してください。答えがわからない場合は、わからないと答えましょう。回答は3文以内で簡潔に。\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate([\n",
    "    HumanMessagePromptTemplate.from_template(template),\n",
    "])\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 5. 実行\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"リポジトリにはどのような役割がありますか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックス: ロード\n",
    "\n",
    "まず、[DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/)を利用してブログ記事のコンテンツをロードします。  \n",
    "[DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/)は、ソースからデータを読み込み、 `Document` のリストを返します。`Document` は `page_content(str)` と `metadata(dict)` を要素に持ちます。\n",
    "\n",
    "今回はブログの記事を読み込むので、[WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base/)を使用します。  \n",
    "これは `urllib` を利用してWebからHTMLをロードし、 `BeautifulSoup` を使用してテキストにパースします。  \n",
    "`bs_kwargs` を利用することで、 `BeautifulSourp` にパラメータを渡すことができます。 ([BeautifulSoupのドキュメント](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup))  \n",
    "今回の場合、 `post-content` `post-title` `post-header` クラスを持つHTMLタグのみ取得し、それ以外は削除します。  \n",
    "\n",
    "## 参考\n",
    "\n",
    "- [各データソースにおけるDocumentLoaderの使い方](https://python.langchain.com/docs/how_to/#document-loaders)\n",
    "- [DocumentLoaders ドキュメント](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "  - [BaseLoader (DocumentLoadersのインターフェース)](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs length: 1\n",
      "content length: 43131\n",
      "metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# post-title, post-content, post-headerのみ取得\n",
    "bs4_strainer = bs4.SoupStrainer(class_=[\"post-title\", \"post-content\", \"post-header\"])\n",
    "\n",
    "# ロード\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}  # BeautifulSoupの引数\n",
    ")\n",
    "docs: List[Document] = loader.load()\n",
    "\n",
    "print(f\"docs length: {len(docs)}\")\n",
    "print(f\"content length: {len(docs[0].page_content)}\")\n",
    "print(f\"metadata: {docs[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# 取得したデータを表示\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックス: 分割\n",
    "\n",
    "ドキュメントを1000文字のチャンクに分割し、チャンク間に200文字のオーバーラップ(文とそれに関連する重要な文脈が分離してしまう可能性を軽減する)を設けます。  \n",
    "[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/) を使用し、各チャンクが適切なサイズになるまで、改行などの一般的な区切り文字を使用してドキュメントを再帰的に分割します。これは、一般的なテキストを使う場合の推奨テキストスプリッタです。\n",
    "\n",
    "`add_start_index=True` を設定し、分割されたDocumentが元の文章の何文字目から始まるのかを示すメタデータ属性 `start_index` を保持するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs length: 66\n",
      "content length (index=9): 960\n",
      "metadata (index=9): {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 6095}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits: List[Document] = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"docs length: {len(all_splits)}\")\n",
    "print(f\"content length (index=9): {len(all_splits[9].page_content)}\")\n",
    "print(f\"metadata (index=9): {all_splits[9].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックス: 保存\n",
    "\n",
    "[Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/)ベクトルストアと[OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai/)モデルを使って、ドキュメント分割をベクトル化し保存します。\n",
    "\n",
    "## 参考\n",
    "\n",
    "- Embeddings\n",
    "  - [Embeddingsの使い方](https://python.langchain.com/docs/how_to/embed_text/)\n",
    "  - [Embeddingモデル一覧](https://python.langchain.com/docs/integrations/text_embedding/)\n",
    "- VectorStore\n",
    "  - [VectorStoreの使い方](https://python.langchain.com/docs/how_to/vectorstores/)\n",
    "  - [VectorStore一覧](https://python.langchain.com/docs/integrations/vectorstores/)\n",
    "  - [インターフェース](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検索と生成: 検索\n",
    "\n",
    "最も一般的なRetrieverは [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html) で、ベクトルストアの類似検索機能を利用して検索を行います。  \n",
    "どの `VectorStore` も `VectorStore.as_retriever()` メソッドで簡単に `Retriever` オブジェクトを取得することができます。\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [vectorestoreをretrieverとして使うには？](https://python.langchain.com/docs/how_to/vectorstore_retriever/)\n",
    "- [Retrieverの使い方一覧](https://python.langchain.com/docs/how_to/#retrievers)\n",
    "- [Retriever一覧](https://python.langchain.com/docs/integrations/retrievers/)\n",
    "- [BaseRetrieverインターフェース](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved docs: 5\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "retriever: VectorStoreRetriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "retrieved_docs: List[Document] = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(f\"retrieved docs: {len(retrieved_docs)}\")\n",
    "print(retrieved_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検索と生成: 生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、LangChainの[prompt hub](https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=1f22096d-1ee0-451f-9572-c1bdbc2ef487)にチェックインされているRAG用プロンプトを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# プロンプトのテンプレートを取得\n",
    "# https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=1f22096d-1ee0-451f-9572-c1bdbc2ef487\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_messages)\n",
    "print(example_messages[0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved using techniques like Chain of Thought (CoT) and Tree of Thoughts, which encourage systematic reasoning and exploration of multiple possibilities. By doing so, it enhances understanding and execution efficiency for both human and AI agents."
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# retrieverから取得したDocumentを文字列に成形\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自前のプロンプトを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate planning and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts enhance this process by enabling models to reason step-by-step and explore multiple possibilities. This approach helps improve performance on complex tasks and clarifies the model's reasoning. Thanks for asking!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# モデル\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# プロンプト\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = ChatPromptTemplate([\n",
    "    HumanMessagePromptTemplate.from_template(template),\n",
    "])\n",
    "\n",
    "# retrieverから取得したDocumentを文字列に成形\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
